{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data \n",
    "data = pandas.read_csv('co2_emissions_data.csv')\n",
    "\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 : data analysis \n",
    "\n",
    "\n",
    "# check nulls in the data\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# get description of the data to detect the scale of the data \n",
    "numericFeatures = data.select_dtypes(include=[np.number])\n",
    "print(numericFeatures.describe())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.pairplot(data, diag_kind='hist')\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the heatmap works numerical values so we well select the numerical values only\n",
    "numeric_cols = data.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_cols.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='summer')\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make                                0\n",
    "Model                               0\n",
    "Vehicle Class                       0\n",
    "Engine Size(L)                      0\n",
    "Cylinders                           0\n",
    "Transmission                        0\n",
    "Fuel Type                           0\n",
    "Fuel Consumption City (L/100 km)    0\n",
    "Fuel Consumption Hwy (L/100 km)     0\n",
    "Fuel Consumption Comb (L/100 km)    0\n",
    "Fuel Consumption Comb (mpg)         0\n",
    "CO2 Emissions(g/km)                 0\n",
    "Emission Class      \"\"\"\n",
    "\n",
    "#Step 2  Preprocessing\n",
    "feature1  = 'Cylinders'\n",
    "feature2 = 'Fuel Consumption Comb (L/100 km)'\n",
    "output = 'CO2 Emissions(g/km)'\n",
    "alpha = 0.1   # learning rate 0.1\n",
    "maxIterations = 1000\n",
    "\n",
    "\n",
    "\n",
    "data = shuffle(data, random_state=100)  # shuffle the data\n",
    "\n",
    "trainingData, testData = train_test_split(data, test_size=0.3, random_state=100) # split the data into training and testing data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "trainingData = pandas.DataFrame(scaler.fit_transform(trainingData[[feature1, feature2, output]]), columns=[feature1, feature2, output])\n",
    "testData = pandas.DataFrame(scaler.fit_transform(testData[[feature1, feature2, output]]), columns=[feature1, feature2, output])\n",
    "\n",
    "x1 = trainingData[feature1]\n",
    "x2 = trainingData[feature2] \n",
    "\n",
    "\n",
    "print(x1.describe())\n",
    "print(x2.describe())\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# x1Normalized = scaler.fit_transform(x1.values.reshape(-1, 1))\n",
    "# x2Noralized = scaler.fit_transform(x2.values.reshape(-1, 1))\n",
    "\n",
    "# print(x1Normalized[:5])\n",
    "# print(x2Noralized[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Training the model\n",
    "\n",
    "x = np.column_stack((x1, x2))\n",
    "y = trainingData[output]  ## TODO : check if we need to normalize the y values\n",
    "\n",
    "# y = (y - y.mean()) / y.std() \n",
    "\n",
    "# yMean = y.mean()\n",
    "# yStd = y.std()\n",
    "\n",
    "# y = (y - yMean) / yStd\n",
    "\n",
    "# def denormalize_y(yNormalized, yMean, yStd):\n",
    "#     return yNormalized * yStd + yMean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(x[:5])\n",
    "# print(y[:5])\n",
    "\n",
    "costs = []\n",
    "\n",
    "def  fitGD(x, y, alpha, maxIterations):\n",
    "    x = np.c_[np.ones(x.shape[0]), x]  # Add a column of ones to x for the bias term\n",
    "    thetas = np.random.rand(x.shape[1]) # initialize thetas with zeros based on the number of features in the data + 1 for the bias\n",
    "    for i in range(maxIterations):\n",
    "        h = np.dot( x, thetas)\n",
    "        for j in range(len(thetas)):\n",
    "            partialDerivative = (1/len(y)) * np.sum((h - y) * x[:, j])\n",
    "            thetas[j] = thetas[j] - alpha * partialDerivative\n",
    "            cost = (1/len(y)) * np.sum(np.square(h - y))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    return thetas\n",
    "\n",
    "\n",
    "thetas = fitGD(x, y, alpha, maxIterations)\n",
    "\n",
    "plot.plot(costs)\n",
    "plot.xlabel('Number of Iterations')\n",
    "plot.ylabel('Cost')\n",
    "plot.show()\n",
    "\n",
    "print(thetas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.8812404252877073\n",
      "[-1.17576158e-16  2.93912439e-01  6.88954267e-01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the test data\n",
    "x1Test = testData[feature1]\n",
    "x2Test = testData[feature2]\n",
    "\n",
    "# x1TestNormalized = scaler.transform(x1Test.values.reshape(-1, 1))\n",
    "# x2TestNormalized = scaler.transform(x2Test.values.reshape(-1, 1))\n",
    "\n",
    "xTest = np.column_stack((x1Test, x2Test))\n",
    "xTest = np.c_[np.ones(xTest.shape[0]), xTest]  # Add a column of ones to x_test for the bias term\n",
    "\n",
    "# Predict the CO2 emissions for the test set\n",
    "yTest = testData[output]\n",
    "\n",
    "\n",
    "\n",
    "yPred = np.dot( xTest, thetas)\n",
    "\n",
    "\n",
    "\n",
    "# # Inverse transform only the output column\n",
    "# yPred = pandas.DataFrame(scaler.inverse_transform(testData[[feature1, feature2, output]])[:, 2].reshape(-1, 1), columns=[output])\n",
    "# yTest = pandas.DataFrame(scaler.inverse_transform(testData[[feature1, feature2, output]])[:, 2].reshape(-1, 1), columns=[output])\n",
    "\n",
    "# print(yPred[:5])\n",
    "# print(yTest[:5])\n",
    "\n",
    "\n",
    "# yPred = denormalize_y(yPred, yMean, yStd)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2 = r2_score(yTest, yPred)\n",
    "print(f'R2 score: {r2}')\n",
    "\n",
    "\n",
    "\n",
    "print(thetas)\n",
    "\n",
    "## TODO check this : The logistic regression model should be a stochastic gradient\n",
    "## descent classifier\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
